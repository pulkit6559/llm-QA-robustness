{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eb1c645-f3cb-4332-bedb-c1a78b1bc6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install --compile -r requirements.txt --no-cache-dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fb8795e-2474-4fc5-a82a-7d2a0f4e6e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nyt_1990.txt\n",
      "nyt_2000.txt\n",
      "nyt_2010.txt\n",
      "nyt_2020.txt\n",
      "\n",
      "gzip: stdin: unexpected end of file\n",
      "tar: Unexpected EOF in archive\n",
      "tar: Unexpected EOF in archive\n",
      "tar: Error is not recoverable: exiting now\n"
     ]
    }
   ],
   "source": [
    "!tar -xzvf datasets/nyt_with10k_every10.tar.gz -C datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02def6da-2ed6-42f7-9c84-93689edc10f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bertenv\t\t\t\t  sentence_time_prediction.py\n",
      "bert_model.py\t\t\t  tempobert.ipynb\n",
      "configuration_temporal_models.py  temporal_data_collator.py\n",
      "datasets\t\t\t  temporal_text_dataset.py\n",
      "data_utils.py\t\t\t  test_bert.py\n",
      "hf_utils.py\t\t\t  tokenization_tempobert_fast.py\n",
      "LICENSE\t\t\t\t  tokenization_utils_base.py\n",
      "myenv\t\t\t\t  tokenization_utils_fast.py\n",
      "output\t\t\t\t  train.ipynb\n",
      "README.md\t\t\t  train_tempobert.py\n",
      "requirements.txt\t\t  utils.py\n",
      "semantic_change_detection.py\n"
     ]
    }
   ],
   "source": [
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0b4808-82c3-4739-9c21-2c2abbca9f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-18 14:49:45.546\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mhf_utils\u001b[0m:\u001b[36minit_run\u001b[0m:\u001b[36m194\u001b[0m - \u001b[33m\u001b[1mProcess rank: -1, device: cuda:0, n_gpu: 1, distributed training: False\u001b[0m\n",
      "\u001b[32m2023-10-18 14:49:45.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhf_utils\u001b[0m:\u001b[36minit_run\u001b[0m:\u001b[36m199\u001b[0m - \u001b[1mModelArguments(model_name_or_path='bert-base-uncased', config_overrides=None, config_name=None, tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True, freeze_layers=False, hidden_size=768, num_hidden_layers=12, tokenizer='bert-base', time_embedding_type='prepend_token', gradient_checkpointing=False)\u001b[0m\n",
      "\u001b[32m2023-10-18 14:49:45.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhf_utils\u001b[0m:\u001b[36minit_run\u001b[0m:\u001b[36m199\u001b[0m - \u001b[1mDataTrainingArguments(dataset_name='temporal_text_dataset.py', dataset_config_name=None, train_preprocessed=None, validation_preprocessed=None, train_path='datasets/nyt_with10k_every10', validation_path=None, overwrite_cache=False, max_seq_length=128, preprocessing_num_workers=None, mlm_probability=0.15, time_mlm_probability=0.9, line_by_line=False, pad_to_max_length=False, times=None, words_for_vocab_file=None, corpus_name=None)\u001b[0m\n",
      "\u001b[32m2023-10-18 14:49:45.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhf_utils\u001b[0m:\u001b[36minit_run\u001b[0m:\u001b[36m199\u001b[0m - \u001b[1mTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=output/runs/Oct18_14-49-45_torch-cuda-568fb8d4d6-kqdjc,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "output_dir=output/TempoBERT_2023-10-18_14-49-45,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=output,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=False,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=output,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\u001b[0m\n",
      "\u001b[32m2023-10-18 14:49:45.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_tempobert\u001b[0m:\u001b[36m440\u001b[0m - \u001b[1mLoaded 4 time points from datasets/nyt_with10k_every10.\u001b[0m\n",
      "\u001b[32m2023-10-18 14:49:45.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_tempobert\u001b[0m:\u001b[36m451\u001b[0m - \u001b[1mTraining TempoBERT from a pretrained bert-base-uncased model\u001b[0m\n",
      "Downloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 406kB/s]\n",
      "Downloading: 100%|███████████████████████████| 440M/440M [00:07<00:00, 55.7MB/s]\n",
      "[WARNING|modeling_utils.py:1505] 2023-10-18 14:49:56,642 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 18.4kB/s]\n",
      "Downloading: 100%|███████████████████████████| 232k/232k [00:00<00:00, 1.08MB/s]\n",
      "Downloading: 100%|███████████████████████████| 466k/466k [00:00<00:00, 1.49MB/s]\n",
      "\u001b[32m2023-10-18 14:50:00.398\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhf_utils\u001b[0m:\u001b[36mload_pretrained_model\u001b[0m:\u001b[36m123\u001b[0m - \u001b[1mLoaded a pretrained model from bert-base-uncased\u001b[0m\n",
      "\u001b[32m2023-10-18 14:50:00.994\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m345\u001b[0m - \u001b[1mLoading dataset files...\u001b[0m\n",
      "Using custom data configuration default-12c2733da9f62c6c\n",
      "Downloading and preparing dataset temporal_text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/jovyan/.cache/huggingface/datasets/temporal_text/default-12c2733da9f62c6c/0.0.0/d61618ea9db23e8edd5ca94e76dd20393e5f7c01699c956624a40cd91fabb97f...\n",
      "100%|███████████████████████████████████████████| 4/4 [00:00<00:00, 5917.89it/s]\n",
      "100%|████████████████████████████████████████████| 4/4 [00:00<00:00, 424.90it/s]\n",
      "Dataset temporal_text downloaded and prepared to /home/jovyan/.cache/huggingface/datasets/temporal_text/default-12c2733da9f62c6c/0.0.0/d61618ea9db23e8edd5ca94e76dd20393e5f7c01699c956624a40cd91fabb97f. Subsequent calls will reuse this data.\n",
      "\u001b[32m2023-10-18 14:50:02.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m352\u001b[0m - \u001b[1mLoaded dataset of 367,137 rows. Preprocessing...\u001b[0m\n",
      "Parameter 'function'=<function tokenize_dataset_concat.<locals>.tokenize_function at 0x7fb3d4125ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Running tokenizer on every text in dataset:   0%|       | 0/368 [00:00<?, ?ba/s]Token indices sequence length is longer than the specified maximum sequence length for this model (146 > 128). Running this sequence through the model will result in indexing errors\n",
      "Running tokenizer on every text in dataset: 100%|█| 368/368 [01:00<00:00,  6.07b\n",
      "Grouping texts in chunks of 128: 100%|████████| 368/368 [02:07<00:00,  2.88ba/s]\n",
      "\u001b[32m2023-10-18 14:53:10.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m398\u001b[0m - \u001b[1mPreprocessed dataset! 92,505 rows. Elapsed time: 0:03:08.232576\u001b[0m\n",
      "[INFO|trainer.py:421] 2023-10-18 14:53:12,085 >> Using amp fp16 backend\n",
      "\u001b[32m2023-10-18 14:53:12.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_tempobert\u001b[0m:\u001b[36m536\u001b[0m - \u001b[1mTraining TempoBERT... Output folder: output/TempoBERT_2023-10-18_14-49-45\u001b[0m\n",
      "[INFO|trainer.py:1170] 2023-10-18 14:53:12,096 >> ***** Running training *****\n",
      "[INFO|trainer.py:1171] 2023-10-18 14:53:12,096 >>   Num examples = 92505\n",
      "[INFO|trainer.py:1172] 2023-10-18 14:53:12,096 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1173] 2023-10-18 14:53:12,096 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1174] 2023-10-18 14:53:12,096 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1175] 2023-10-18 14:53:12,096 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1176] 2023-10-18 14:53:12,096 >>   Total optimization steps = 34692\n",
      "{'loss': 2.762, 'learning_rate': 4.928225527499136e-05, 'epoch': 0.04}          \n",
      "  1%|▌                                    | 500/34692 [01:36<1:48:58,  5.23it/s][INFO|trainer.py:1925] 2023-10-18 14:54:48,430 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-500\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 14:54:48,434 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 14:54:49,718 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-500/pytorch_model.bin\n",
      "{'loss': 2.5776, 'learning_rate': 4.856306929551482e-05, 'epoch': 0.09}         \n",
      "  3%|█                                   | 1000/34692 [03:16<1:47:31,  5.22it/s][INFO|trainer.py:1925] 2023-10-18 14:56:28,861 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-1000\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 14:56:28,865 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 14:56:30,202 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-1000/pytorch_model.bin\n",
      "{'loss': 2.5762, 'learning_rate': 4.784388331603828e-05, 'epoch': 0.13}         \n",
      "  4%|█▌                                  | 1500/34692 [04:56<1:46:06,  5.21it/s][INFO|trainer.py:1925] 2023-10-18 14:58:08,508 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-1500\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 14:58:08,511 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 14:58:09,807 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-1500/pytorch_model.bin\n",
      "{'loss': 2.5367, 'learning_rate': 4.712325608209386e-05, 'epoch': 0.17}         \n",
      "  6%|██                                  | 2000/34692 [06:36<1:44:35,  5.21it/s][INFO|trainer.py:1925] 2023-10-18 14:59:48,144 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-2000\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 14:59:48,147 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 14:59:49,414 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-2000/pytorch_model.bin\n",
      "{'loss': 2.4682, 'learning_rate': 4.640262884814943e-05, 'epoch': 0.22}         \n",
      "  7%|██▌                                 | 2500/34692 [08:15<1:42:49,  5.22it/s][INFO|trainer.py:1925] 2023-10-18 15:01:27,675 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-2500\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:01:27,679 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:01:28,970 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-2500/pytorch_model.bin\n",
      "{'loss': 2.4675, 'learning_rate': 4.5682001614205007e-05, 'epoch': 0.26}        \n",
      "  9%|███                                 | 3000/34692 [09:55<1:42:20,  5.16it/s][INFO|trainer.py:1925] 2023-10-18 15:03:07,364 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-3000\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:03:07,368 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:03:08,681 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-3000/pytorch_model.bin\n",
      "{'loss': 2.4259, 'learning_rate': 4.4961374380260576e-05, 'epoch': 0.3}         \n",
      " 10%|███▋                                | 3500/34692 [11:34<1:39:33,  5.22it/s][INFO|trainer.py:1925] 2023-10-18 15:04:47,024 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-3500\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:04:47,028 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:04:48,371 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-3500/pytorch_model.bin\n",
      "{'loss': 2.4398, 'learning_rate': 4.424074714631616e-05, 'epoch': 0.35}         \n",
      " 12%|████▏                               | 4000/34692 [13:14<1:37:59,  5.22it/s][INFO|trainer.py:1925] 2023-10-18 15:06:26,802 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-4000\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:06:26,805 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:06:28,112 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-4000/pytorch_model.bin\n",
      "{'loss': 2.4104, 'learning_rate': 4.352011991237173e-05, 'epoch': 0.39}         \n",
      " 13%|████▋                               | 4500/34692 [14:54<1:36:18,  5.23it/s][INFO|trainer.py:1925] 2023-10-18 15:08:06,367 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-4500\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:08:06,371 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:08:07,637 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-4500/pytorch_model.bin\n",
      "{'loss': 2.3953, 'learning_rate': 4.2799492678427306e-05, 'epoch': 0.43}        \n",
      " 14%|█████▏                              | 5000/34692 [16:33<1:34:40,  5.23it/s][INFO|trainer.py:1925] 2023-10-18 15:09:45,914 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-5000\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:09:45,917 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:09:47,240 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-5000/pytorch_model.bin\n",
      "{'loss': 2.3924, 'learning_rate': 4.2078865444482876e-05, 'epoch': 0.48}        \n",
      " 16%|█████▋                              | 5500/34692 [18:13<1:33:13,  5.22it/s][INFO|trainer.py:1925] 2023-10-18 15:11:25,525 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-5500\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:11:25,529 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:11:26,828 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-5500/pytorch_model.bin\n",
      "{'loss': 2.4017, 'learning_rate': 4.135967946500635e-05, 'epoch': 0.52}         \n",
      " 17%|██████▏                             | 6000/34692 [19:52<1:31:37,  5.22it/s][INFO|trainer.py:1925] 2023-10-18 15:13:05,084 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-6000\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:13:05,087 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-6000/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:13:06,383 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-6000/pytorch_model.bin\n",
      "{'loss': 2.3613, 'learning_rate': 4.063905223106192e-05, 'epoch': 0.56}         \n",
      " 19%|██████▋                             | 6500/34692 [21:32<1:29:59,  5.22it/s][INFO|trainer.py:1925] 2023-10-18 15:14:44,845 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-6500\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:14:44,850 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-6500/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:14:46,144 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-6500/pytorch_model.bin\n",
      "{'loss': 2.3865, 'learning_rate': 3.9918424997117495e-05, 'epoch': 0.61}        \n",
      " 20%|███████▎                            | 7000/34692 [23:12<1:29:02,  5.18it/s][INFO|trainer.py:1925] 2023-10-18 15:16:24,394 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-7000\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:16:24,400 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-7000/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:16:25,675 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-7000/pytorch_model.bin\n",
      "{'loss': 2.3676, 'learning_rate': 3.9197797763173065e-05, 'epoch': 0.65}        \n",
      " 22%|███████▊                            | 7500/34692 [24:52<1:26:49,  5.22it/s][INFO|trainer.py:1925] 2023-10-18 15:18:04,101 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-7500\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:18:04,106 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-7500/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:18:05,486 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-7500/pytorch_model.bin\n",
      "{'loss': 2.3649, 'learning_rate': 3.847717052922864e-05, 'epoch': 0.69}         \n",
      " 23%|████████▎                           | 8000/34692 [26:31<1:25:08,  5.23it/s][INFO|trainer.py:1925] 2023-10-18 15:19:44,030 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-8000\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:19:44,034 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-8000/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:19:45,296 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-8000/pytorch_model.bin\n",
      "{'loss': 2.369, 'learning_rate': 3.775654329528422e-05, 'epoch': 0.74}          \n",
      " 25%|████████▊                           | 8500/34692 [28:11<1:23:40,  5.22it/s][INFO|trainer.py:1925] 2023-10-18 15:21:23,744 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-8500\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:21:23,747 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-8500/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:21:25,075 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-8500/pytorch_model.bin\n",
      "{'loss': 2.3497, 'learning_rate': 3.7035916061339794e-05, 'epoch': 0.78}        \n",
      " 26%|█████████▎                          | 9000/34692 [29:51<1:22:06,  5.21it/s][INFO|trainer.py:1925] 2023-10-18 15:23:03,539 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-9000\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:23:03,543 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-9000/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:23:04,832 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-9000/pytorch_model.bin\n",
      "{'loss': 2.3649, 'learning_rate': 3.6315288827395364e-05, 'epoch': 0.82}        \n",
      " 27%|█████████▊                          | 9500/34692 [31:31<1:20:23,  5.22it/s][INFO|trainer.py:1925] 2023-10-18 15:24:43,132 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-9500\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:24:43,135 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-9500/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:24:44,482 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-9500/pytorch_model.bin\n",
      "{'loss': 2.3501, 'learning_rate': 3.559610284791883e-05, 'epoch': 0.86}         \n",
      " 29%|██████████                         | 10000/34692 [33:10<1:18:46,  5.22it/s][INFO|trainer.py:1925] 2023-10-18 15:26:22,845 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-10000\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:26:22,848 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-10000/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:26:24,151 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-10000/pytorch_model.bin\n",
      "{'loss': 2.3613, 'learning_rate': 3.487547561397441e-05, 'epoch': 0.91}         \n",
      " 30%|██████████▌                        | 10500/34692 [34:50<1:17:19,  5.21it/s][INFO|trainer.py:1925] 2023-10-18 15:28:02,475 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-10500\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:28:02,479 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-10500/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:28:03,750 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-10500/pytorch_model.bin\n",
      "{'loss': 2.33, 'learning_rate': 3.415484838002998e-05, 'epoch': 0.95}           \n",
      " 32%|███████████                        | 11000/34692 [36:29<1:15:49,  5.21it/s][INFO|trainer.py:1925] 2023-10-18 15:29:42,030 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-11000\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:29:42,034 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-11000/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:29:43,339 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-11000/pytorch_model.bin\n",
      "{'loss': 2.3332, 'learning_rate': 3.343422114608555e-05, 'epoch': 0.99}         \n",
      " 33%|███████████▌                       | 11500/34692 [38:09<1:14:12,  5.21it/s][INFO|trainer.py:1925] 2023-10-18 15:31:21,837 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-11500\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:31:21,840 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-11500/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:31:23,164 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-11500/pytorch_model.bin\n",
      "{'loss': 2.2882, 'learning_rate': 3.271359391214113e-05, 'epoch': 1.04}         \n",
      " 35%|████████████                       | 12000/34692 [39:49<1:12:30,  5.22it/s][INFO|trainer.py:1925] 2023-10-18 15:33:01,646 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-12000\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:33:01,650 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-12000/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:33:02,926 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-12000/pytorch_model.bin\n",
      "{'loss': 2.2791, 'learning_rate': 3.1992966678196706e-05, 'epoch': 1.08}        \n",
      " 36%|████████████▌                      | 12500/34692 [41:29<1:11:00,  5.21it/s][INFO|trainer.py:1925] 2023-10-18 15:34:41,469 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-12500\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:34:41,473 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-12500/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:34:42,727 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-12500/pytorch_model.bin\n",
      "{'loss': 2.2546, 'learning_rate': 3.127233944425228e-05, 'epoch': 1.12}         \n",
      " 37%|█████████████                      | 13000/34692 [43:09<1:09:19,  5.21it/s][INFO|trainer.py:1925] 2023-10-18 15:36:21,225 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-13000\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:36:21,228 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-13000/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:36:22,523 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-13000/pytorch_model.bin\n",
      "{'loss': 2.2806, 'learning_rate': 3.055171221030785e-05, 'epoch': 1.17}         \n",
      " 39%|█████████████▌                     | 13500/34692 [44:48<1:07:43,  5.22it/s][INFO|trainer.py:1925] 2023-10-18 15:38:01,083 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-13500\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:38:01,086 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-13500/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:38:02,392 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-13500/pytorch_model.bin\n",
      "{'loss': 2.2616, 'learning_rate': 2.9831084976363426e-05, 'epoch': 1.21}        \n",
      " 40%|██████████████                     | 14000/34692 [46:29<1:06:54,  5.15it/s][INFO|trainer.py:1925] 2023-10-18 15:39:41,352 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-14000\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:39:41,356 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-14000/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:39:42,629 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-14000/pytorch_model.bin\n",
      "{'loss': 2.2553, 'learning_rate': 2.9116222760290557e-05, 'epoch': 1.25}        \n",
      " 42%|██████████████▋                    | 14500/34692 [48:09<1:05:10,  5.16it/s][INFO|trainer.py:1925] 2023-10-18 15:41:21,279 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-14500\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:41:21,282 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-14500/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:41:22,596 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-14500/pytorch_model.bin\n",
      "{'loss': 2.2375, 'learning_rate': 2.839559552634613e-05, 'epoch': 1.3}          \n",
      " 43%|███████████████▏                   | 15000/34692 [49:49<1:03:01,  5.21it/s][INFO|trainer.py:1925] 2023-10-18 15:43:01,611 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-15000\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:43:01,614 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-15000/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:43:02,917 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-15000/pytorch_model.bin\n",
      "{'loss': 2.2494, 'learning_rate': 2.767496829240171e-05, 'epoch': 1.34}         \n",
      " 45%|███████████████▋                   | 15500/34692 [51:29<1:01:40,  5.19it/s][INFO|trainer.py:1925] 2023-10-18 15:44:41,691 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-15500\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:44:41,696 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-15500/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:44:42,958 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-15500/pytorch_model.bin\n",
      "{'loss': 2.2583, 'learning_rate': 2.6954341058457284e-05, 'epoch': 1.38}        \n",
      " 46%|█████████████████                    | 16000/34692 [53:09<59:56,  5.20it/s][INFO|trainer.py:1925] 2023-10-18 15:46:21,679 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-16000\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:46:21,683 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-16000/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:46:22,981 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-16000/pytorch_model.bin\n",
      "{'loss': 2.2434, 'learning_rate': 2.6233713824512857e-05, 'epoch': 1.43}        \n",
      " 48%|█████████████████▌                   | 16500/34692 [54:49<58:19,  5.20it/s][INFO|trainer.py:1925] 2023-10-18 15:48:01,670 >> Saving model checkpoint to output/TempoBERT_2023-10-18_14-49-45/checkpoint-16500\n",
      "[INFO|configuration_utils.py:379] 2023-10-18 15:48:01,674 >> Configuration saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-16500/config.json\n",
      "[INFO|modeling_utils.py:1001] 2023-10-18 15:48:02,945 >> Model weights saved in output/TempoBERT_2023-10-18_14-49-45/checkpoint-16500/pytorch_model.bin\n",
      " 48%|█████████████████▋                   | 16638/34692 [55:19<57:51,  5.20it/s]"
     ]
    }
   ],
   "source": [
    "!python train_tempobert.py  --model_name_or_path bert-base-uncased \\\n",
    "    --train_path datasets/nyt_with10k_every10  \\\n",
    "    --do_train \\\n",
    "    --output_dir output \\\n",
    "    --time_embedding_type prepend_token \\\n",
    "    --time_mlm_probability 0.9 \\\n",
    "    --max_seq_length 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83b0cfe-180c-421f-aecf-1f8a12a6a6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_time_prediction import sentence_time_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eac5ee0-6dd9-4510-809b-09049035a77f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
